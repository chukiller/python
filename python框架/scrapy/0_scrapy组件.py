# 1.Scrapy引擎（Engine）：用来控制整个系统的数据处理
# 2.调度器（Scheduler）：调度器从引擎接受请求并排序列入队列，并在引擎发出请求后返还给它们
# 3.下载器（Downloader）：下载器的主要职责是抓取网页并将网页内容返还给蜘蛛（Spiders）
# 4.蜘蛛程序（Spiders）：蜘蛛是用户自定义的用来解析网页并抓取特定URL的类，每个蜘蛛都能处理一个域名或一组域名，
#                       简单的说就是用来定义特定网站的抓取和解析规则的模块
# 5.数据管道（Item Pipeline）：管道的主要职责是负责处理有蜘蛛从网页中抽取的数据条目，它的主要任务是清理、验证和
#                             存储数据。当页面被蜘蛛解析后，将被发送到数据管道，并经过几个特定的次序处理数据。
#                             每个数据管道都是一个python类，它们获取了数据条目并执行对数据条目进行处理的方法，
#                             同时还需要确定是否需要在数据管道中继续执行下一步或是直接丢弃掉不处理。数据管道
#                             通常执行的任务有：清理HTML数据、验证解析到的数据（检查条目是否包含必要的字段）、
#                             检查是不是重复数据（如果重复就丢弃）、将解析到的数据存储到数据库中。
# 6.中间件（Middlewares）：中间件是介于引擎和其他组件之间的一个钩子框架，主要是为了提供自定义的代码来拓展
#                         Scrapy的功能，包括下载器之间件和蜘蛛之间件

